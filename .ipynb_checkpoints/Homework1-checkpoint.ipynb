{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1, COMS 4995_005, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Possible Score 120 Points  (100 + 20 Extra Credits)\n",
    "----\n",
    "\n",
    "# Part1: (Basic Neural Nework) (70 Points)\n",
    "\n",
    "### Part 1.1:\n",
    "\n",
    "- Divide the training data into 80% training set and 20% validation set. \n",
    "- Implement the functions in the ipython notebook so that you can train your network. \n",
    "- Your code should take network structure, training data, hyperparameters and generate validation set accuracy.\n",
    "- Use Relu activation for intermediate layers and use cross entropy loss after taking softmax on the output of the final layer.\n",
    "\n",
    "If you get all the things mentioned above working - **60 Points**\n",
    "\n",
    "### Part 1.2:\n",
    "\n",
    "Test your model accuracy on test set. If it is more than **47%**, you will get an additional score of **10 points**\n",
    "\n",
    "# Part 2: (Regularization) (30 Points)\n",
    "\n",
    "### Part 2.1 (15 Points) :\n",
    "\n",
    "Modify code to add L2 regularization. Report the validation accuracy.\n",
    "\n",
    "You should get a validation and test accuracy of more than the one reported in Part-1\n",
    "\n",
    "### Part 2.2 (15 Points):\n",
    "\n",
    "You should get a validation and test accuracy crossing **50%**\n",
    "\n",
    "\n",
    "# Extra Credit (20 Points)\n",
    "\n",
    "Show your excitement on deep learning! Top **3 scorers** will get these **20 points**\n",
    "\n",
    "(Hints) Boost your accuracy by trying out: \n",
    "- Dropout Regularization\n",
    "- Batch Normalization\n",
    "- Other optimizers like Adam\n",
    "- Learning Rate Decay\n",
    "- Data Augmentation \n",
    "- Different Initializations for weights like Xaviers etc.\n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Guidelines:\n",
    "\n",
    "1. Write your code in **Python 3**.\n",
    "2. **DONOT** import any other packages.\n",
    "3. Click **https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz** -> download **cifar-10-python.tar.gz** -> extract as **cifar-10-python**\n",
    "4. Ensure that **this ipython notebook** and **cifar-10-python** folder are in the same folder.\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "1. Run this ipython notebook once and submit this file. Ensure that the outputs are printed properly. We will first see the outputs, if there are no outputs, we may not run the notebook at all.\n",
    "2. Training on the **test data** is considered cheating. If we find any clue of that happening, then we will disqualify the submission and it will be reported accordingly.\n",
    "3. Each team member needs to separately submit the the file named uni.ipynb on courseworks.\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "Team Member1 (Name,UNI): \n",
    "\n",
    "Team Member2 (Name, UNI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Do not import other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of a Fully Connected Network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    You can add more functions in this class, and also modify inputs and outputs of each function\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dim):\n",
    "        \"\"\"\n",
    "        layer_dim: List containing layer dimensions. \n",
    "        \n",
    "        Code:\n",
    "        Initialize weight and biases for each layer\n",
    "        \"\"\"\n",
    "#         self.W = None\n",
    "#         self.b = None\n",
    "\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        i = 0\n",
    "        while i<(len(layer_dimensions)-1):\n",
    "            self.W.append((np.random.random((layer_dim[i+1],layer_dim[i]))-0.5)*0.02)\n",
    "            self.b.append(np.zeros((layer_dim[i+1],1)))\n",
    "            i = i+1\n",
    "        self.num_layers = len(layer_dimensions)-1\n",
    "#         self.drop_prob = drop_prob\n",
    "#         self.reg_lambda = reg_lambda\n",
    "        self.training_mode = 0\n",
    "    \n",
    "    def feedforward(self,X):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        Returns output of the neural network for input X. Also returns cache, which contains outputs of \n",
    "        intermediate layers which would be useful during backprop.\n",
    "        \"\"\"       \n",
    "        cache = {\"d_activation\":[None]*(len(self.W)), \"r_activation\":[None]*(len(self.b)), \"dm\":[None]*(len(self.W)-1)}\n",
    "        cache[\"d_activation\"][0] = X\n",
    "        cache[\"r_activation\"][0] = X\n",
    "        j = 1\n",
    "        while j<len(self.W):\n",
    "#             print(self.W[j-1].shape, cache[\"d_activation\"][j-1].shape)\n",
    "            cache[\"r_activation\"][j] = self.relu_forward(self.affineForward(self.W[j-1], cache[\"d_activation\"][j-1], self.b[j-1]))\n",
    "            cache[\"d_activation\"][j] = cache[\"r_activation\"][j]\n",
    "            j = j+1\n",
    "            \n",
    "        At = self.affineForward(self.W[j-1],cache[\"d_activation\"][j-1],self.b[j-1])\n",
    "        return At, cache\n",
    "    \n",
    "    def loss_function(self, At, Y):\n",
    "        \"\"\"\n",
    "        At is the output of the last layer, returned by feedforward.\n",
    "        Y contains true labels for this batch.\n",
    "        this function takes softmax the last layer's output and calculates loss.\n",
    "        the gradient of loss with respect to the activations of the last layer are also returned by this function.\n",
    "        \"\"\"\n",
    "        exp_At = np.exp(At)\n",
    "        prob = exp_At/np.sum(exp_At, axis=0, keepdims=True)\n",
    "        logprob = -np.log(prob[Y,range(At.shape[1])])\n",
    "        cost = np.sum(logprob)/At.shape[1]\n",
    "        # gradient of cost\n",
    "        dAt = prob\n",
    "        dAt[Y,range(At.shape[1])] -= 1\n",
    "        dAt /= At.shape[1]\n",
    "        return cost, dAt\n",
    "    \n",
    "    def train(self, X, Y, max_iters=5000, batch_size=100, learning_rate=0.01, lambd=0,validate_every=200):\n",
    "        \"\"\"\n",
    "        X: (3072 dimensions, 50000 examples) (Cifar train data)\n",
    "        Y: (1 dimension, 50000 examples)\n",
    "        lambd: the hyperparameter corresponding to L2 regularization\n",
    "        \n",
    "        Divide X, Y into train(80%) and val(20%), during training do evaluation on val set\n",
    "        after every validate_every iterations and in the end use the parameters corresponding to the best\n",
    "        val set to test on the Cifar test set. Print the accuracy that is calculated on the val set during \n",
    "        training. Also print the final test accuracy. Ensure that these printed values can be seen in the .ipynb file you\n",
    "        submit.\n",
    "        \n",
    "        Expected Functionality: \n",
    "        This function will call functions feedforward, backprop and update_params. \n",
    "        Also, evaluate on the validation set for tuning the hyperparameters.\n",
    "        \"\"\"\n",
    "        x_train, x_test = X[:, :40000], X[:, 40000:]\n",
    "        y_train, y_test = Y[:, :40000], Y[:, 40000:]\n",
    "#         print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "#         print(\"hello world!\")\n",
    "        i = 0\n",
    "        cost = 200\n",
    "        while i < max_iters and cost > 0.1:\n",
    "            # get minibatch\n",
    "            x_t,y_t = self.get_batch(x_train,y_train,batch_size)\n",
    "            y_t = y_t.astype(int)\n",
    "#             forward prop\n",
    "#             self.training_mode = 1 #for training mode\n",
    "            At,cache = self.feedforward(x_t)\n",
    "#             print(At.shape)\n",
    "#             self.training_mode = 0 #for testing mode\n",
    "#             compute loss\n",
    "            cost,dAL = self.loss_function(At,y_t)\n",
    "            # compute gradients\n",
    "            gradients = self.backprop(dAct=dAL,loss=cost,cache=cache)\n",
    "            # update weights and biases based on gradient\n",
    "#             print(\"The info of gradients is\", gradients['dweights'][0].shape)\n",
    "#             print(\"The info of W is\", self.W[0].shape)\n",
    "            self.updateParameters(gradients,learning_rate)\n",
    "            if i % validate_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                # print (\"iteration:%i\" % (i))\n",
    "#                 print (\"cost:%.2f, train accuracy:%.2f, validation accuracy:%.2f\" % (cost,np.mean(self.predict(X_tr) == y_tr),np.mean(self.predict(X_val) == y_val)))\n",
    "                print(\"Step is:\", i, \"| Cost is:\", cost)\n",
    "                if i % 5000 == 0 and i != 0:\n",
    "                    learning_rate /= 3\n",
    "                    batch_size = min(batch_size*2, 40000)\n",
    "            i += 1\n",
    "    \n",
    "    def affineForward(self, W, A, b):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        return np.dot(W, A)+b\n",
    "        \n",
    "    \n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Backward pass for the affine layer.\n",
    "        dA_prev: gradient from the next layer.\n",
    "        cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        dA = np.dot(self.W[self.num_layers].T, dA_prev)\n",
    "        dW = np.dot(cache[\"d_activation\"][self.num_layers], dA_prev.T,)\n",
    "        db = np.sum(dA_prev, axis=1, keepdims=True)\n",
    "        return dA, dW, db\n",
    "        \n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass of relu activation\n",
    "        \"\"\"\n",
    "        return np.maximum(0, X)\n",
    "        \n",
    "    def relu_backward(self, dx, cached_x):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        backward pass for relu activation\n",
    "        \"\"\"\n",
    "        dx[cached_x[\"r_activation\"][self.num_layers] <= 0] = 0\n",
    "        return dx\n",
    "    \n",
    "    def get_batch(self, X, Y, batch_size):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        given the full training data (X, Y), return batches for each iteration of forward and backward prop.\n",
    "        \"\"\"\n",
    "        choices = np.random.choice(range(40000), batch_size, replace=False)\n",
    "        x_batch = np.zeros((X.shape[0], batch_size))\n",
    "        y_batch = np.zeros((Y.shape[0], batch_size))\n",
    "        for j, choice in enumerate(choices):\n",
    "            x_batch[:, j] = X[:, choice]\n",
    "            y_batch[:, j] = Y[:, choice]      \n",
    "        return x_batch, y_batch \n",
    "        \n",
    "    def backprop(self, loss, cache, dAct):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        returns gradients for all parameters in the network.\n",
    "        dAct is the gradient of loss with respect to the output of final layer of the network.\n",
    "        \"\"\"\n",
    "        gradients = {\"dweights\":[None]*(len(self.W)),\"dbiases\":[None]*(len(self.b))}\n",
    "        i = len(self.W)-1\n",
    "        self.num_layers = i\n",
    "        temp_dA, temp_dW, temp_db = self.affineBackward(dAct, cache)\n",
    "        gradients[\"dweights\"][i] = temp_dW\n",
    "        gradients[\"dbiases\"][i] = temp_db\n",
    "        temp_dA_next = temp_dA\n",
    "        temp_dA_next = self.relu_backward(temp_dA_next, cache)\n",
    "        i -= 1\n",
    "        while i>=0:\n",
    "            self.num_layers = i\n",
    "            temp_dA, temp_dW, temp_db = self.affineBackward(temp_dA_next, cache)\n",
    "            gradients[\"dweights\"][i] = temp_dW\n",
    "            gradients[\"dbiases\"][i] = temp_db\n",
    "            temp_dA_next = temp_dA\n",
    "            temp_dA_next = self.relu_backward(temp_dA_next, cache)      \n",
    "            i=i-1\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def updateParameters(self, gradients, learning_rate):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        use gradients returned by backprop to update the parameters.\n",
    "        \"\"\"\n",
    "        i=0\n",
    "        while i<len(self.W):\n",
    "#             print(gradients[\"dweights\"][i].shape)\n",
    "            self.W[i] += -learning_rate*gradients[\"dweights\"][i].T\n",
    "            self.b[i] += -learning_rate*gradients[\"dbiases\"][i]\n",
    "            i = i+1\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        '''\n",
    "        X: X_test (3472 dimensions, 10000 examples)\n",
    "        Y: Y_test (1 dimension, 10000 examples)\n",
    "        \n",
    "        Expected Functionality: \n",
    "        print accuracy on test set\n",
    "        '''\n",
    "        print(\"evaluation started\")\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0.0\n",
    "        Y_test = Y_test.astype(int)\n",
    "        i = 0\n",
    "        while i < len(X_test[0]):\n",
    "            curr_x = X_test[:, i:i+1]\n",
    "#             print(curr_x.shape)\n",
    "            y_predicted, _ = self.feedforward(curr_x)\n",
    "#             print(y_predicted, len(Y_test), y_predicted.shape)\n",
    "#             print(Y_test[0][i], len(Y_test), Y_test.shape)\n",
    "            if Y_test[0][i] == y_predicted.argmax():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            i += 1\n",
    "            if i % 50 == 0:\n",
    "                print(\"Step:\", i, \"Correctness:\", correct/total)\n",
    "            \n",
    "        return correct / total\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \n",
    "    def unpickle(self, file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        '''\n",
    "        loads training data: 50,000 examples with 3072 features\n",
    "        '''\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        for i in range(1, 6):\n",
    "            pickleFile = self.unpickle('./datasets/data_batch_{}'.format(i))\n",
    "            dataX = pickleFile[b'data']\n",
    "            dataY = pickleFile[b'labels']\n",
    "            if type(X_train) is np.ndarray:\n",
    "                X_train = np.concatenate((X_train, dataX))\n",
    "                Y_train = np.concatenate((Y_train, dataY))\n",
    "            else:\n",
    "                X_train = dataX\n",
    "                Y_train = dataY\n",
    "\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "        return X_train.T, Y_train.T\n",
    "\n",
    "    def load_test_data(self):\n",
    "        '''\n",
    "        loads testing data: 10,000 examples with 3072 features\n",
    "        '''\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        pickleFile = self.unpickle('./datasets/test_batch')\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_test) is np.ndarray:\n",
    "            X_test = np.concatenate((X_test, dataX))\n",
    "            Y_test = np.concatenate((Y_test, dataY))\n",
    "        else:\n",
    "            X_test = np.array(dataX)\n",
    "            Y_test = np.array(dataY)\n",
    "\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        return X_test.T, Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train: (3072, 50000) -> 50000 examples, 3072 features\n",
      "Y_Train: (1, 50000) -> 50000 examples, 1 features\n",
      "X_Test: (3072, 10000) -> 10000 examples, 3072 features\n",
      "Y_Test: (1, 10000) -> 10000 examples, 1 features\n",
      "(3072, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train = Loader().load_train_data()\n",
    "X_test, Y_test = Loader().load_test_data()\n",
    "\n",
    "print(\"X_Train: {} -> {} examples, {} features\".format(X_train.shape, X_train.shape[1], X_train.shape[0]))\n",
    "print(\"Y_Train: {} -> {} examples, {} features\".format(Y_train.shape, Y_train.shape[1], Y_train.shape[0]))\n",
    "print(\"X_Test: {} -> {} examples, {} features\".format(X_test.shape, X_test.shape[1], X_test.shape[0]))\n",
    "print(\"Y_Test: {} -> {} examples, {} features\".format(Y_test.shape, Y_test.shape[1], Y_test.shape[0]))\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step is: 0 | Cost is: 2.32708963119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-125f70ee8ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 3072 is the input feature size, 10 is the number of outputs in the final layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFCN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnectedNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mFCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# lambd, the L2 regularization penalty hyperparamter will be 0 for this part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# print accuracy on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14acc3a38634>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, max_iters, batch_size, learning_rate, lambd, validate_every)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m#             forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m#             self.training_mode = 1 #for training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mAt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;31m#             print(At.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m#             self.training_mode = 0 #for testing mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14acc3a38634>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#             print(self.W[j-1].shape, cache[\"d_activation\"][j-1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r_activation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffineForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_activation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_activation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r_activation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14acc3a38634>\u001b[0m in \u001b[0;36maffineForward\u001b[0;34m(self, W, A, b)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maffine\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mWA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malong\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcache\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dimensions = [3072,1000,100, 10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions)\n",
    "FCN.train(X_train, Y_train, max_iters=200000, batch_size=500, learning_rate=0.001, lambd=0,validate_every=50)\n",
    "# lambd, the L2 regularization penalty hyperparamter will be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test, Y_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [3072,..,.., 10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions)\n",
    "FCN.train(X_train, Y_train, max_iters=10000, batch_size=200, learning_rate=0.01, lambd=0.1,validate_every=200)\n",
    "# lambd, the L2 regularization penalty hyperparamter will not be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
