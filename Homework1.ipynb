{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1, COMS 4995_005, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Possible Score 120 Points  (100 + 20 Extra Credits)\n",
    "----\n",
    "\n",
    "# Part1: (Basic Neural Nework) (70 Points)\n",
    "\n",
    "### Part 1.1:\n",
    "\n",
    "- Divide the training data into 80% training set and 20% validation set. \n",
    "- Implement the functions in the ipython notebook so that you can train your network. \n",
    "- Your code should take network structure, training data, hyperparameters and generate validation set accuracy.\n",
    "- Use Relu activation for intermediate layers and use cross entropy loss after taking softmax on the output of the final layer.\n",
    "\n",
    "If you get all the things mentioned above working - **60 Points**\n",
    "\n",
    "### Part 1.2:\n",
    "\n",
    "Test your model accuracy on test set. If it is more than **47%**, you will get an additional score of **10 points**\n",
    "\n",
    "# Part 2: (Regularization) (30 Points)\n",
    "\n",
    "### Part 2.1 (15 Points) :\n",
    "\n",
    "Modify code to add L2 regularization. Report the validation accuracy.\n",
    "\n",
    "You should get a validation and test accuracy of more than the one reported in Part-1\n",
    "\n",
    "### Part 2.2 (15 Points):\n",
    "\n",
    "You should get a validation and test accuracy crossing **50%**\n",
    "\n",
    "\n",
    "# Extra Credit (20 Points)\n",
    "\n",
    "Show your excitement on deep learning! Top **3 scorers** will get these **20 points**\n",
    "\n",
    "(Hints) Boost your accuracy by trying out: \n",
    "- Dropout Regularization\n",
    "- Batch Normalization\n",
    "- Other optimizers like Adam\n",
    "- Learning Rate Decay\n",
    "- Data Augmentation \n",
    "- Different Initializations for weights like Xaviers etc.\n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Guidelines:\n",
    "\n",
    "1. Write your code in **Python 3**.\n",
    "2. **DONOT** import any other packages.\n",
    "3. Click **https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz** -> download **cifar-10-python.tar.gz** -> extract as **cifar-10-python**\n",
    "4. Ensure that **this ipython notebook** and **cifar-10-python** folder are in the same folder.\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "1. Run this ipython notebook once and submit this file. Ensure that the outputs are printed properly. We will first see the outputs, if there are no outputs, we may not run the notebook at all.\n",
    "2. Training on the **test data** is considered cheating. If we find any clue of that happening, then we will disqualify the submission and it will be reported accordingly.\n",
    "3. Each team member needs to separately submit the the file named uni.ipynb on courseworks.\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "Team Member1 (Name,UNI): \n",
    "\n",
    "Team Member2 (Name, UNI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Do not import other packages\n",
    "# ys3031 Simon Sun | pz2232 Ping Zhu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of a Fully Connected Network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    You can add more functions in this class, and also modify inputs and outputs of each function\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dim, lambd=0):\n",
    "        \"\"\"\n",
    "        layer_dim: List containing layer dimensions. \n",
    "        \n",
    "        Code:\n",
    "        Initialize weight and biases for each layer\n",
    "        \"\"\"\n",
    "\n",
    "#         np.random.seed(1)\n",
    "        \n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        i = 0\n",
    "        while i<(len(layer_dim)-1):\n",
    "            self.W.append((np.random.random((layer_dim[i+1],layer_dim[i]))-0.5)*0.02)\n",
    "            self.b.append(np.zeros((layer_dim[i+1],1)))\n",
    "            i = i+1\n",
    "        self.num_layers = len(layer_dim)-1\n",
    "        \n",
    "        self.lambd = lambd\n",
    "        \n",
    "    def feedforward(self,X):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        Returns output of the neural network for input X. Also returns cache, which contains outputs of \n",
    "        intermediate layers which would be useful during backprop.\n",
    "        \"\"\"       \n",
    "        cache_dict = {\"d\":[None]*(len(self.W)), \"r\":[None]*(len(self.b))}\n",
    "        cache_dict[\"d\"][0] = X\n",
    "        cache_dict[\"r\"][0] = X\n",
    "        j = 1\n",
    "        while j<len(self.W):\n",
    "            cache_dict[\"r\"][j] = self.relu_forward(self.affineForward(cache_dict[\"d\"][j-1], self.W[j-1], self.b[j-1]))\n",
    "            cache_dict[\"d\"][j] = cache_dict[\"r\"][j]\n",
    "            j = j+1\n",
    "            \n",
    "        At = self.affineForward(cache_dict[\"d\"][j-1], self.W[j-1], self.b[j-1])\n",
    "        return At, cache_dict\n",
    "    \n",
    "    def loss_function(self, At, Y):\n",
    "        \"\"\"\n",
    "        At is the output of the last layer, returned by feedforward.\n",
    "        Y contains true labels for this batch.\n",
    "        this function takes softmax the last layer's output and calculates loss.\n",
    "        the gradient of loss with respect to the activations of the last layer are also returned by this function.\n",
    "        \"\"\"\n",
    "        exp_output = np.exp(At)\n",
    "        probabilities = exp_output / np.sum(exp_output, axis=0, keepdims=True)\n",
    "        log_probabilities = -np.log(probabilities[Y,range(At.shape[1])])\n",
    "        loss = np.sum(log_probabilities) / At.shape[1]\n",
    "        dAt = probabilities\n",
    "        dAt[Y,range(At.shape[1])] -= 1\n",
    "        dAt /= At.shape[1]\n",
    "        return loss, dAt\n",
    "    \n",
    "    def train(self, X, Y, max_iters=5000, batch_size=100, learning_rate=0.01, validate_every=200, \n",
    "              loss_until=0.4, accuracy_until=0.51):\n",
    "        \"\"\"\n",
    "        X: (3072 dimensions, 50000 examples) (Cifar train data)\n",
    "        Y: (1 dimension, 50000 examples)\n",
    "        lambd: the hyperparameter corresponding to L2 regularization\n",
    "        \n",
    "        Divide X, Y into train(80%) and val(20%), during training do evaluation on val set\n",
    "        after every validate_every iterations and in the end use the parameters corresponding to the best\n",
    "        val set to test on the Cifar test set. Print the accuracy that is calculated on the val set during \n",
    "        training. Also print the final test accuracy. Ensure that these printed values can be seen in the .ipynb file you\n",
    "        submit.\n",
    "        \n",
    "        Expected Functionality: \n",
    "        This function will call functions feedforward, backprop and update_params. \n",
    "        Also, evaluate on the validation set for tuning the hyperparameters.\n",
    "        \"\"\"\n",
    "        x_train, x_test = X[:, :40000], X[:, 40000:]\n",
    "        y_train, y_test = Y[:, :40000], Y[:, 40000:]\n",
    "        i = 0\n",
    "        loss = 200\n",
    "        loss_list = []\n",
    "        acc_list = []\n",
    "        iter_list = []\n",
    "        accuracy = -1\n",
    "        \n",
    "        print(\"traning started\")\n",
    "        while i < max_iters+1 and loss > loss_until and accuracy < accuracy_until:\n",
    "            x_batch,y_batch = self.get_batch(x_train,y_train,batch_size)\n",
    "            y_batch = y_batch.astype(int)\n",
    "            At,cache = self.feedforward(x_batch)\n",
    "            loss,dAt = self.loss_function(At,y_batch)\n",
    "            gradients = self.backprop(loss=loss,cache=cache,dAct=dAt,)\n",
    "            self.updateParameters(gradients,learning_rate)\n",
    "            if i % validate_every == 0:\n",
    "                accuracy = self.evaluate(x_test, y_test, print_every=False)\n",
    "                print(\"Step is:\", i, \"| Loss is:\", loss, \"| Accuracy is:\", accuracy)\n",
    "                loss_list.append(loss)\n",
    "                acc_list.append(accuracy)\n",
    "                iter_list.append(i)\n",
    "                if i % 5000 == 0 and i != 0:\n",
    "                    learning_rate /= 2\n",
    "            i += 1\n",
    "           \n",
    "        print(\"training ended\")\n",
    "        print(\"----------------------------\")\n",
    "        print(\"plotting started\")\n",
    "        plt.plot(iter_list, loss_list, 'bo')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('loss over iterations')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(iter_list, acc_list, 'bo')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('accuracy over iterations')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"plotting ended\")\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        return np.dot(W, A)+b\n",
    "        \n",
    "    \n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Backward pass for the affine layer.\n",
    "        dA_prev: gradient from the next layer.\n",
    "        cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        dA = np.dot(self.W[self.num_layers].T, dA_prev)\n",
    "        dW = np.dot(cache[\"d\"][self.num_layers], dA_prev.T,)\n",
    "        db = np.sum(dA_prev, axis=1, keepdims=True)\n",
    "        return dA, dW, db\n",
    "        \n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass of relu activation\n",
    "        \"\"\"\n",
    "        return np.maximum(0, X)\n",
    "        \n",
    "    def relu_backward(self, dx, cached_x):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        backward pass for relu activation\n",
    "        \"\"\"\n",
    "        dx[cached_x[\"r\"][self.num_layers] <= 0] = 0\n",
    "        return dx\n",
    "    \n",
    "    def get_batch(self, X, Y, batch_size):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        given the full training data (X, Y), return batches for each iteration of forward and backward prop.\n",
    "        \"\"\"\n",
    "        choices = np.random.choice(range(40000), batch_size, replace=False)\n",
    "        x_batch = np.zeros((X.shape[0], batch_size))\n",
    "        y_batch = np.zeros((Y.shape[0], batch_size))\n",
    "        for j, choice in enumerate(choices):\n",
    "            x_batch[:, j] = X[:, choice]\n",
    "            y_batch[:, j] = Y[:, choice]      \n",
    "        return x_batch, y_batch \n",
    "        \n",
    "    def backprop(self, loss, cache, dAct):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        returns gradients for all parameters in the network.\n",
    "        dAct is the gradient of loss with respect to the output of final layer of the network.\n",
    "        \"\"\"\n",
    "        gradients = {\"d_weight\":[None]*(len(self.W)),\"d_bias\":[None]*(len(self.b))}\n",
    "        i = len(self.W)-1\n",
    "        self.num_layers = i\n",
    "        temp_dA, temp_dW, temp_db = self.affineBackward(dAct, cache)\n",
    "        gradients[\"d_weight\"][i], gradients[\"d_bias\"][i] = temp_dW, temp_db\n",
    "        temp_dA_next = self.relu_backward(temp_dA, cache)\n",
    "        i -= 1\n",
    "        while i>=0:\n",
    "            self.num_layers = i\n",
    "            temp_dA, temp_dW, temp_db = self.affineBackward(temp_dA_next, cache)\n",
    "            gradients[\"d_weight\"][i] = temp_dW\n",
    "            gradients[\"d_bias\"][i] = temp_db\n",
    "            temp_dA_next = temp_dA\n",
    "            temp_dA_next = self.relu_backward(temp_dA_next, cache)      \n",
    "            i -= 1\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def updateParameters(self, gradients, learning_rate):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        use gradients returned by backprop to update the parameters.\n",
    "        \"\"\"\n",
    "        i=0\n",
    "        while i<len(self.W):\n",
    "            self.W[i] += -learning_rate*gradients[\"d_weight\"][i].T\n",
    "            self.b[i] += -learning_rate*gradients[\"d_bias\"][i]\n",
    "            i = i+1\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test, print_every=50):\n",
    "        '''\n",
    "        X: X_test (3072 dimensions, 10000 examples)\n",
    "        Y: Y_test (1 dimension, 10000 examples)\n",
    "        \n",
    "        Expected Functionality: \n",
    "        print accuracy on test set\n",
    "        '''\n",
    "        if print_every:\n",
    "            print(\"----------------------------\")\n",
    "            print(\"evaluation started\")\n",
    "            \n",
    "        correct = 0.0\n",
    "        Y_test = Y_test.astype(int)\n",
    "        i = 0\n",
    "        while i < len(X_test[0]):\n",
    "            curr_x = X_test[:, i:i+1]\n",
    "            y_predicted, _ = self.feedforward(curr_x)\n",
    "            if Y_test[0][i] == y_predicted.argmax():\n",
    "                correct += 1\n",
    "            i += 1\n",
    "            if print_every and i % print_every == 0:\n",
    "                print(\"Step:\", i, \"Correctness:\", correct/i)\n",
    "        \n",
    "        total = i\n",
    "        return correct / total\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \n",
    "    def unpickle(self, file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        '''\n",
    "        loads training data: 50,000 examples with 3072 features\n",
    "        '''\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        for i in range(1, 6):\n",
    "            pickleFile = self.unpickle('./datasets/data_batch_{}'.format(i))\n",
    "            dataX = pickleFile[b'data']\n",
    "            dataY = pickleFile[b'labels']\n",
    "            if type(X_train) is np.ndarray:\n",
    "                X_train = np.concatenate((X_train, dataX))\n",
    "                Y_train = np.concatenate((Y_train, dataY))\n",
    "            else:\n",
    "                X_train = dataX\n",
    "                Y_train = dataY\n",
    "\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "        return X_train.T, Y_train.T\n",
    "\n",
    "    def load_test_data(self):\n",
    "        '''\n",
    "        loads testing data: 10,000 examples with 3072 features\n",
    "        '''\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        pickleFile = self.unpickle('./datasets/test_batch')\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_test) is np.ndarray:\n",
    "            X_test = np.concatenate((X_test, dataX))\n",
    "            Y_test = np.concatenate((Y_test, dataY))\n",
    "        else:\n",
    "            X_test = np.array(dataX)\n",
    "            Y_test = np.array(dataY)\n",
    "\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        return X_test.T, Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train: (3072, 50000) -> 50000 examples, 3072 features\n",
      "Y_Train: (1, 50000) -> 50000 examples, 1 features\n",
      "X_Test: (3072, 10000) -> 10000 examples, 3072 features\n",
      "Y_Test: (1, 10000) -> 10000 examples, 1 features\n",
      "(3072, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train = Loader().load_train_data()\n",
    "X_test, Y_test = Loader().load_test_data()\n",
    "\n",
    "print(\"X_Train: {} -> {} examples, {} features\".format(X_train.shape, X_train.shape[1], X_train.shape[0]))\n",
    "print(\"Y_Train: {} -> {} examples, {} features\".format(Y_train.shape, Y_train.shape[1], Y_train.shape[0]))\n",
    "print(\"X_Test: {} -> {} examples, {} features\".format(X_test.shape, X_test.shape[1], X_test.shape[0]))\n",
    "print(\"Y_Test: {} -> {} examples, {} features\".format(Y_test.shape, Y_test.shape[1], Y_test.shape[0]))\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning started\n",
      "Step is: 0 | Loss is: 2.34151224374 | Accuracy is: 0.093\n",
      "Step is: 100 | Loss is: 1.94757104594 | Accuracy is: 0.2953\n",
      "Step is: 200 | Loss is: 1.85700748803 | Accuracy is: 0.3377\n",
      "Step is: 300 | Loss is: 1.87957342264 | Accuracy is: 0.3469\n",
      "Step is: 400 | Loss is: 1.78794490655 | Accuracy is: 0.3468\n",
      "Step is: 500 | Loss is: 1.71540532744 | Accuracy is: 0.384\n",
      "Step is: 600 | Loss is: 1.77460372912 | Accuracy is: 0.3662\n",
      "Step is: 700 | Loss is: 1.65635004217 | Accuracy is: 0.3979\n",
      "Step is: 800 | Loss is: 1.56543452148 | Accuracy is: 0.4142\n",
      "Step is: 900 | Loss is: 1.64025536154 | Accuracy is: 0.4071\n",
      "Step is: 1000 | Loss is: 1.63924066859 | Accuracy is: 0.4248\n",
      "Step is: 1100 | Loss is: 1.55256325132 | Accuracy is: 0.4105\n",
      "Step is: 1200 | Loss is: 1.56871018066 | Accuracy is: 0.4175\n",
      "Step is: 1300 | Loss is: 1.49301072372 | Accuracy is: 0.4423\n",
      "Step is: 1400 | Loss is: 1.56110881296 | Accuracy is: 0.4429\n",
      "Step is: 1500 | Loss is: 1.50280700279 | Accuracy is: 0.4492\n",
      "Step is: 1600 | Loss is: 1.49453850298 | Accuracy is: 0.4441\n",
      "Step is: 1700 | Loss is: 1.54835865141 | Accuracy is: 0.4319\n",
      "Step is: 1800 | Loss is: 1.47212891774 | Accuracy is: 0.4167\n",
      "Step is: 1900 | Loss is: 1.53134476619 | Accuracy is: 0.4536\n",
      "Step is: 2000 | Loss is: 1.51671630899 | Accuracy is: 0.4621\n",
      "Step is: 2100 | Loss is: 1.46760815298 | Accuracy is: 0.4611\n",
      "Step is: 2200 | Loss is: 1.42959606543 | Accuracy is: 0.455\n",
      "Step is: 2300 | Loss is: 1.43990924394 | Accuracy is: 0.4603\n",
      "Step is: 2400 | Loss is: 1.42344240727 | Accuracy is: 0.4643\n",
      "Step is: 2500 | Loss is: 1.36461299951 | Accuracy is: 0.4659\n",
      "Step is: 2600 | Loss is: 1.47377449484 | Accuracy is: 0.4558\n",
      "Step is: 2700 | Loss is: 1.43680488864 | Accuracy is: 0.4763\n",
      "Step is: 2800 | Loss is: 1.40251466537 | Accuracy is: 0.4825\n",
      "Step is: 2900 | Loss is: 1.2956908293 | Accuracy is: 0.4661\n",
      "Step is: 3000 | Loss is: 1.38379670144 | Accuracy is: 0.4662\n",
      "Step is: 3100 | Loss is: 1.32360274644 | Accuracy is: 0.4814\n",
      "Step is: 3200 | Loss is: 1.31147676485 | Accuracy is: 0.4949\n",
      "Step is: 3300 | Loss is: 1.31681748035 | Accuracy is: 0.4818\n",
      "Step is: 3400 | Loss is: 1.34002092711 | Accuracy is: 0.4539\n",
      "Step is: 3500 | Loss is: 1.28053179349 | Accuracy is: 0.4848\n",
      "Step is: 3600 | Loss is: 1.27784445996 | Accuracy is: 0.4907\n",
      "Step is: 3700 | Loss is: 1.24844737484 | Accuracy is: 0.4752\n",
      "Step is: 3800 | Loss is: 1.28281517238 | Accuracy is: 0.4963\n",
      "Step is: 3900 | Loss is: 1.21477188594 | Accuracy is: 0.4905\n",
      "Step is: 4000 | Loss is: 1.30368633925 | Accuracy is: 0.489\n",
      "Step is: 4100 | Loss is: 1.32172824967 | Accuracy is: 0.4782\n",
      "Step is: 4200 | Loss is: 1.30592382851 | Accuracy is: 0.4639\n",
      "Step is: 4300 | Loss is: 1.20865824001 | Accuracy is: 0.473\n",
      "Step is: 4400 | Loss is: 1.20954414247 | Accuracy is: 0.4906\n",
      "Step is: 4500 | Loss is: 1.20820580932 | Accuracy is: 0.4992\n",
      "Step is: 4600 | Loss is: 1.15821111637 | Accuracy is: 0.4958\n",
      "Step is: 4700 | Loss is: 1.40312422796 | Accuracy is: 0.474\n",
      "Step is: 4800 | Loss is: 1.19264195281 | Accuracy is: 0.4919\n",
      "Step is: 4900 | Loss is: 1.24130672471 | Accuracy is: 0.4933\n",
      "Step is: 5000 | Loss is: 1.22560661223 | Accuracy is: 0.4909\n",
      "Step is: 5100 | Loss is: 1.1704842486 | Accuracy is: 0.5179\n",
      "training ended\n",
      "----------------------------\n",
      "plotting started\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgpmZlbjoGBmZjX/Hyxg52iSWsEgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ce99550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9f8BI/i4+2uDy4kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cea3438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting ended\n",
      "----------------------------\n",
      "evaluation started\n",
      "Step: 50 Correctness: 0.52\n",
      "Step: 100 Correctness: 0.58\n",
      "Step: 150 Correctness: 0.5933333333333334\n",
      "Step: 200 Correctness: 0.545\n",
      "Step: 250 Correctness: 0.524\n",
      "Step: 300 Correctness: 0.52\n",
      "Step: 350 Correctness: 0.5285714285714286\n",
      "Step: 400 Correctness: 0.54\n",
      "Step: 450 Correctness: 0.5244444444444445\n",
      "Step: 500 Correctness: 0.516\n",
      "Step: 550 Correctness: 0.5145454545454545\n",
      "Step: 600 Correctness: 0.515\n",
      "Step: 650 Correctness: 0.5184615384615384\n",
      "Step: 700 Correctness: 0.5214285714285715\n",
      "Step: 750 Correctness: 0.524\n",
      "Step: 800 Correctness: 0.52125\n",
      "Step: 850 Correctness: 0.5211764705882352\n",
      "Step: 900 Correctness: 0.5277777777777778\n",
      "Step: 950 Correctness: 0.531578947368421\n",
      "Step: 1000 Correctness: 0.536\n",
      "Step: 1050 Correctness: 0.5352380952380953\n",
      "Step: 1100 Correctness: 0.5363636363636364\n",
      "Step: 1150 Correctness: 0.5373913043478261\n",
      "Step: 1200 Correctness: 0.5366666666666666\n",
      "Step: 1250 Correctness: 0.5368\n",
      "Step: 1300 Correctness: 0.5323076923076923\n",
      "Step: 1350 Correctness: 0.5274074074074074\n",
      "Step: 1400 Correctness: 0.525\n",
      "Step: 1450 Correctness: 0.526896551724138\n",
      "Step: 1500 Correctness: 0.5286666666666666\n",
      "Step: 1550 Correctness: 0.5277419354838709\n",
      "Step: 1600 Correctness: 0.525\n",
      "Step: 1650 Correctness: 0.5212121212121212\n",
      "Step: 1700 Correctness: 0.5247058823529411\n",
      "Step: 1750 Correctness: 0.5234285714285715\n",
      "Step: 1800 Correctness: 0.5238888888888888\n",
      "Step: 1850 Correctness: 0.5232432432432432\n",
      "Step: 1900 Correctness: 0.5221052631578947\n",
      "Step: 1950 Correctness: 0.521025641025641\n",
      "Step: 2000 Correctness: 0.5205\n",
      "Step: 2050 Correctness: 0.5214634146341464\n",
      "Step: 2100 Correctness: 0.5233333333333333\n",
      "Step: 2150 Correctness: 0.5241860465116279\n",
      "Step: 2200 Correctness: 0.5227272727272727\n",
      "Step: 2250 Correctness: 0.524\n",
      "Step: 2300 Correctness: 0.5226086956521739\n",
      "Step: 2350 Correctness: 0.5242553191489362\n",
      "Step: 2400 Correctness: 0.5225\n",
      "Step: 2450 Correctness: 0.5195918367346939\n",
      "Step: 2500 Correctness: 0.5228\n",
      "Step: 2550 Correctness: 0.52\n",
      "Step: 2600 Correctness: 0.5180769230769231\n",
      "Step: 2650 Correctness: 0.5188679245283019\n",
      "Step: 2700 Correctness: 0.5188888888888888\n",
      "Step: 2750 Correctness: 0.5185454545454545\n",
      "Step: 2800 Correctness: 0.5178571428571429\n",
      "Step: 2850 Correctness: 0.5164912280701754\n",
      "Step: 2900 Correctness: 0.516896551724138\n",
      "Step: 2950 Correctness: 0.5183050847457628\n",
      "Step: 3000 Correctness: 0.5173333333333333\n",
      "Step: 3050 Correctness: 0.5177049180327868\n",
      "Step: 3100 Correctness: 0.5170967741935484\n",
      "Step: 3150 Correctness: 0.5161904761904762\n",
      "Step: 3200 Correctness: 0.5159375\n",
      "Step: 3250 Correctness: 0.5156923076923077\n",
      "Step: 3300 Correctness: 0.5166666666666667\n",
      "Step: 3350 Correctness: 0.5170149253731343\n",
      "Step: 3400 Correctness: 0.5176470588235295\n",
      "Step: 3450 Correctness: 0.5156521739130435\n",
      "Step: 3500 Correctness: 0.5142857142857142\n",
      "Step: 3550 Correctness: 0.5163380281690141\n",
      "Step: 3600 Correctness: 0.5161111111111111\n",
      "Step: 3650 Correctness: 0.5153424657534247\n",
      "Step: 3700 Correctness: 0.5159459459459459\n",
      "Step: 3750 Correctness: 0.5170666666666667\n",
      "Step: 3800 Correctness: 0.5168421052631579\n",
      "Step: 3850 Correctness: 0.5179220779220779\n",
      "Step: 3900 Correctness: 0.5187179487179487\n",
      "Step: 3950 Correctness: 0.5182278481012659\n",
      "Step: 4000 Correctness: 0.5185\n",
      "Step: 4050 Correctness: 0.5190123456790123\n",
      "Step: 4100 Correctness: 0.5197560975609756\n",
      "Step: 4150 Correctness: 0.52\n",
      "Step: 4200 Correctness: 0.5204761904761904\n",
      "Step: 4250 Correctness: 0.5207058823529411\n",
      "Step: 4300 Correctness: 0.5202325581395348\n",
      "Step: 4350 Correctness: 0.5204597701149425\n",
      "Step: 4400 Correctness: 0.5225\n",
      "Step: 4450 Correctness: 0.5226966292134831\n",
      "Step: 4500 Correctness: 0.5231111111111111\n",
      "Step: 4550 Correctness: 0.5235164835164835\n",
      "Step: 4600 Correctness: 0.5232608695652174\n",
      "Step: 4650 Correctness: 0.5238709677419355\n",
      "Step: 4700 Correctness: 0.5246808510638298\n",
      "Step: 4750 Correctness: 0.5263157894736842\n",
      "Step: 4800 Correctness: 0.526875\n",
      "Step: 4850 Correctness: 0.5263917525773196\n",
      "Step: 4900 Correctness: 0.5265306122448979\n",
      "Step: 4950 Correctness: 0.5268686868686868\n",
      "Step: 5000 Correctness: 0.5258\n",
      "Step: 5050 Correctness: 0.5257425742574258\n",
      "Step: 5100 Correctness: 0.5241176470588236\n",
      "Step: 5150 Correctness: 0.5246601941747573\n",
      "Step: 5200 Correctness: 0.5238461538461539\n",
      "Step: 5250 Correctness: 0.524\n",
      "Step: 5300 Correctness: 0.5239622641509434\n",
      "Step: 5350 Correctness: 0.5235514018691588\n",
      "Step: 5400 Correctness: 0.5235185185185185\n",
      "Step: 5450 Correctness: 0.5240366972477064\n",
      "Step: 5500 Correctness: 0.524\n",
      "Step: 5550 Correctness: 0.5243243243243243\n",
      "Step: 5600 Correctness: 0.5233928571428571\n",
      "Step: 5650 Correctness: 0.5224778761061947\n",
      "Step: 5700 Correctness: 0.5226315789473684\n",
      "Step: 5750 Correctness: 0.5236521739130435\n",
      "Step: 5800 Correctness: 0.5236206896551724\n",
      "Step: 5850 Correctness: 0.5227350427350428\n",
      "Step: 5900 Correctness: 0.5223728813559322\n",
      "Step: 5950 Correctness: 0.5225210084033614\n",
      "Step: 6000 Correctness: 0.5225\n",
      "Step: 6050 Correctness: 0.5223140495867769\n",
      "Step: 6100 Correctness: 0.521311475409836\n",
      "Step: 6150 Correctness: 0.5221138211382114\n",
      "Step: 6200 Correctness: 0.5216129032258064\n",
      "Step: 6250 Correctness: 0.52112\n",
      "Step: 6300 Correctness: 0.5204761904761904\n",
      "Step: 6350 Correctness: 0.5209448818897637\n",
      "Step: 6400 Correctness: 0.5203125\n",
      "Step: 6450 Correctness: 0.5196899224806202\n",
      "Step: 6500 Correctness: 0.5201538461538462\n",
      "Step: 6550 Correctness: 0.5198473282442748\n",
      "Step: 6600 Correctness: 0.5193939393939394\n",
      "Step: 6650 Correctness: 0.5195488721804511\n",
      "Step: 6700 Correctness: 0.5195522388059701\n",
      "Step: 6750 Correctness: 0.5202962962962963\n",
      "Step: 6800 Correctness: 0.52\n",
      "Step: 6850 Correctness: 0.5202919708029197\n",
      "Step: 6900 Correctness: 0.5194202898550725\n",
      "Step: 6950 Correctness: 0.5192805755395683\n",
      "Step: 7000 Correctness: 0.5184285714285715\n",
      "Step: 7050 Correctness: 0.5188652482269503\n",
      "Step: 7100 Correctness: 0.5184507042253521\n",
      "Step: 7150 Correctness: 0.5184615384615384\n",
      "Step: 7200 Correctness: 0.5179166666666667\n",
      "Step: 7250 Correctness: 0.5173793103448275\n",
      "Step: 7300 Correctness: 0.5184931506849315\n",
      "Step: 7350 Correctness: 0.5185034013605442\n",
      "Step: 7400 Correctness: 0.519054054054054\n",
      "Step: 7450 Correctness: 0.5183892617449665\n",
      "Step: 7500 Correctness: 0.5190666666666667\n",
      "Step: 7550 Correctness: 0.5190728476821193\n",
      "Step: 7600 Correctness: 0.5194736842105263\n",
      "Step: 7650 Correctness: 0.5196078431372549\n",
      "Step: 7700 Correctness: 0.5201298701298701\n",
      "Step: 7750 Correctness: 0.520258064516129\n",
      "Step: 7800 Correctness: 0.5207692307692308\n",
      "Step: 7850 Correctness: 0.5201273885350318\n",
      "Step: 7900 Correctness: 0.5203797468354431\n",
      "Step: 7950 Correctness: 0.519874213836478\n",
      "Step: 8000 Correctness: 0.51975\n",
      "Step: 8050 Correctness: 0.519751552795031\n",
      "Step: 8100 Correctness: 0.5196296296296297\n",
      "Step: 8150 Correctness: 0.5188957055214724\n",
      "Step: 8200 Correctness: 0.5190243902439025\n",
      "Step: 8250 Correctness: 0.5186666666666667\n",
      "Step: 8300 Correctness: 0.5183132530120482\n",
      "Step: 8350 Correctness: 0.5174850299401198\n",
      "Step: 8400 Correctness: 0.5167857142857143\n",
      "Step: 8450 Correctness: 0.5166863905325444\n",
      "Step: 8500 Correctness: 0.5172941176470588\n",
      "Step: 8550 Correctness: 0.5176608187134503\n",
      "Step: 8600 Correctness: 0.5175581395348837\n",
      "Step: 8650 Correctness: 0.5174566473988439\n",
      "Step: 8700 Correctness: 0.5172413793103449\n",
      "Step: 8750 Correctness: 0.5172571428571429\n",
      "Step: 8800 Correctness: 0.5172727272727272\n",
      "Step: 8850 Correctness: 0.5172881355932203\n",
      "Step: 8900 Correctness: 0.5179775280898876\n",
      "Step: 8950 Correctness: 0.518659217877095\n",
      "Step: 9000 Correctness: 0.5185555555555555\n",
      "Step: 9050 Correctness: 0.518121546961326\n",
      "Step: 9100 Correctness: 0.5182417582417582\n",
      "Step: 9150 Correctness: 0.518360655737705\n",
      "Step: 9200 Correctness: 0.5193478260869565\n",
      "Step: 9250 Correctness: 0.5194594594594595\n",
      "Step: 9300 Correctness: 0.5195698924731182\n",
      "Step: 9350 Correctness: 0.5192513368983958\n",
      "Step: 9400 Correctness: 0.5192553191489362\n",
      "Step: 9450 Correctness: 0.5194708994708994\n",
      "Step: 9500 Correctness: 0.519578947368421\n",
      "Step: 9550 Correctness: 0.5193717277486911\n",
      "Step: 9600 Correctness: 0.519375\n",
      "Step: 9650 Correctness: 0.5198963730569948\n",
      "Step: 9700 Correctness: 0.52\n",
      "Step: 9750 Correctness: 0.5196923076923077\n",
      "Step: 9800 Correctness: 0.5198979591836734\n",
      "Step: 9850 Correctness: 0.5198984771573604\n",
      "Step: 9900 Correctness: 0.5198989898989899\n",
      "Step: 9950 Correctness: 0.5193969849246232\n",
      "Step: 10000 Correctness: 0.5186\n",
      "The accuracy over test dataset is 0.5186\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [3072,1000,100, 10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions)\n",
    "FCN.train(X_train, Y_train, max_iters=30000, batch_size=500, learning_rate=0.001, validate_every=100)\n",
    "# lambd, the L2 regularization penalty hyperparamter will be 0 for this part\n",
    "acc = FCN.evaluate(X_test, Y_test)  # print accuracy on test set\n",
    "print(\"The accuracy over test dataset is\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dimensions = [3072,..,.., 10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions)\n",
    "FCN.train(X_train, Y_train, max_iters=10000, batch_size=200, learning_rate=0.01, lambd=0.1,validate_every=200)\n",
    "# lambd, the L2 regularization penalty hyperparamter will not be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
